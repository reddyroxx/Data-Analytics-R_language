{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 5 - Data Analytics</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>Topic: Collaborative Filtering</center></h1>\n",
    "\n",
    "\n",
    "## Team Name: Mean Team\n",
    "\n",
    "\n",
    "Name - 1 : Midhush Manohar T.K.  \n",
    "SRN - 1   : 01FB16ECS208   \n",
    "Name - 2 : Naveen Suresh  \n",
    "SRN - 2   : 01FB16ECS222  \n",
    "Name - 3 : Srikumar Subramanian  \n",
    "SRN - 3   : 01FB16ECS396   \n",
    "\n",
    "\n",
    "*Virtually everyone has had an online experience where a website makes personalized recommendations in hopes of future sales or ongoing traffic. Amazon tells you “Customers Who Bought This Item Also Bought”, Udemy tells you “Students Who Viewed This Course Also Viewed”. And Netflix awarded a 1 million dollar prize to a developer team in 2009, for an algorithm that increased the accuracy of the company’s recommendation system by 10 percent.\n",
    "Recommendation Systems have become such a common part of our everyday lives, and there are a number of methods and techniques involved in building them, Collaborative Filtering being one of the main ones.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rating given by each User\n",
      "\n",
      "         rating\n",
      "user_id        \n",
      "243         7.0\n",
      "244         7.0\n",
      "254         7.0\n",
      "805         8.0\n",
      "1075        8.0\n",
      "\n",
      "Count (Training Dataset): 2517\n",
      "\n",
      "\n",
      "         rating\n",
      "user_id        \n",
      "254        10.0\n",
      "805         8.0\n",
      "1075        5.0\n",
      "1261        7.0\n",
      "1424        8.0\n",
      "\n",
      "Count (Testing Dataset): 848\n"
     ]
    }
   ],
   "source": [
    "# a) Average Rating given by each user\n",
    "\n",
    "train_avg = train_data.groupby('user_id').aggregate({'rating':'mean'})\n",
    "test_avg = test_data.groupby('user_id').aggregate({'rating':'mean'})\n",
    "\n",
    "print('Average Rating given by each User')\n",
    "print('')\n",
    "print(train_avg.head())\n",
    "print('')\n",
    "print('Count (Training Dataset): ' + str(len(train_avg)))\n",
    "print('\\n')\n",
    "print(test_avg.head())\n",
    "print('')\n",
    "print('Count (Testing Dataset): ' + str(len(test_avg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bookwise Average of Rating on Training Data\n",
      "                                                      rating\n",
      "book                                                        \n",
      "1st to Die: A Novel                                 7.607843\n",
      "A Heartbreaking Work of Staggering Genius           7.488889\n",
      "A Is for Alibi (Kinsey Millhone Mysteries (Pape...  7.560000\n",
      "A Lesson Before Dying (Vintage Contemporaries (...  7.411765\n",
      "A Man Named Dave: A Story of Triumph and Forgiv...  7.885714\n",
      "\n",
      "100\n",
      "\n",
      "\n",
      "Bookwise Average of Rating on Testing Data\n",
      "                                                      rating\n",
      "book                                                        \n",
      "1st to Die: A Novel                                 7.000000\n",
      "A Heartbreaking Work of Staggering Genius           6.444444\n",
      "A Is for Alibi (Kinsey Millhone Mysteries (Pape...  7.562500\n",
      "A Lesson Before Dying (Vintage Contemporaries (...  8.000000\n",
      "A Man Named Dave: A Story of Triumph and Forgiv...  5.666667\n",
      "\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# b) Average Rating given for each item\n",
    "\n",
    "train_item_avg = train_data.groupby('book').aggregate({'rating':'mean'})\n",
    "test_item_avg = test_data.groupby('book').aggregate({'rating':'mean'})\n",
    "\n",
    "print(\"Bookwise Average of Rating on Training Data\")\n",
    "print(train_item_avg.head())\n",
    "print('')\n",
    "print(len(train_item_avg))\n",
    "print('\\n')\n",
    "print(\"Bookwise Average of Rating on Testing Data\")\n",
    "print(test_item_avg.head())\n",
    "print('')\n",
    "print(len(test_item_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on assigning overall average rating to each user_id/book pair is 1.7813364070210418\n"
     ]
    }
   ],
   "source": [
    "# c) Function to compute the Root Mean Squared Error\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    mse = np.mean((y - y_pred)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# RMSE calculation on assigning overall average rating to each user/item pair\n",
    "\n",
    "# pred = train_data.groupby(['user_id', 'book'])['rating'].mean()\n",
    "train_data_new = train_data.copy(deep = True)\n",
    "train_data_new['rating_pred'] = (train_data_new.groupby(['user_id', 'book']).transform(lambda x: train_data['rating'].mean()))\n",
    "# Each user_id/book pair is unique.\n",
    "rmse_rating = rmse(train_data_new['rating'], train_data_new['rating_pred'])\n",
    "print('RMSE on assigning overall average rating to each user_id/book pair is ' + str(rmse_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity (Train Matrix): 1.5778357559945388 %\n",
      "Sparsity (Test Matrix): 0.6546644844517169 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mimos\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\mimos\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# d) Convert the train and the test data into a matrix. These matrices, in this format, will be used as input to the \n",
    "# functions defined for further predictions. \n",
    "\n",
    "train_rows = train_data.user_id.unique()\n",
    "train_cols = train_data['book'].unique()\n",
    "train_matrix_data = train_data[['user_id', 'book', 'rating']]\n",
    "\n",
    "train_idict = dict(zip(train_cols, range(len(train_cols))))\n",
    "train_udict = dict(zip(train_rows, range(len(train_rows))))\n",
    "\n",
    "train_matrix_data.user_id = [train_udict[i] for i in train_matrix_data.user_id]\n",
    "train_matrix_data['book'] = [train_idict[i] for i in train_matrix_data['book']]\n",
    "\n",
    "train_matrix = train_matrix_data.as_matrix()\n",
    "\n",
    "# print(train_matrix)\n",
    "\n",
    "\n",
    "test_rows = test_data.user_id.unique()\n",
    "test_cols = test_data['book'].unique()\n",
    "test_matrix_data = test_data[['user_id', 'book', 'rating']]\n",
    "\n",
    "test_idict = dict(zip(test_cols, range(len(test_cols))))\n",
    "test_udict = dict(zip(test_rows, range(len(test_rows))))\n",
    "\n",
    "test_matrix_data.user_id = [test_udict[i] for i in test_matrix_data.user_id]\n",
    "test_matrix_data['book'] = [test_idict[i] for i in test_matrix_data['book']]\n",
    "\n",
    "test_matrix = test_matrix_data.as_matrix()\n",
    "\n",
    "# print(test_matrix)\n",
    "\n",
    "# Calculate the Sparsity of these matrices.\n",
    "\n",
    "train_sparsity = 1.0 - np.count_nonzero(train_matrix)/train_matrix.size\n",
    "print('Sparsity (Train Matrix): ' + str(train_sparsity*100) + ' %')\n",
    "\n",
    "test_sparsity = 1.0 - np.count_nonzero(test_matrix)/test_matrix.size\n",
    "print('Sparsity (Test Matrix): ' + str(test_sparsity*100) + ' %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Train Dataset): 1.1444196071250665\n",
      "RMSE (Test Dataset): 2.2847566119422518\n"
     ]
    }
   ],
   "source": [
    "# e) Write a function to predict in the following manner:\n",
    "# For any (user i, item j), assign it with: \n",
    "#      (mean rating given by user i) + (mean rating received by item j) – (average rating over the entire dataset)\n",
    "#      i.e., umean[i] + imean[j] – amean\n",
    "\n",
    "def predict_naive(user, item):\n",
    "    prediction = item_mean[item] + user_mean[user] - avg_mean\n",
    "    return prediction\n",
    "\n",
    "naive = np.zeros((len(train_rows), len(train_cols)))\n",
    "for row in train_matrix:\n",
    "    naive[row[0], row[1]] = row[2]\n",
    "\n",
    "avg_mean = np.mean(naive[naive != 0])\n",
    "user_mean = sum(naive.T)/sum((naive != 0).T)\n",
    "item_mean = sum(naive)/sum(naive != 0)\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for row in train_matrix:\n",
    "    user, item, actual = row[0], row[1], row[2]\n",
    "    predictions.append(predict_naive(user, item))\n",
    "    targets.append(actual)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "targets = np.array(targets)\n",
    "rmse_value_train = rmse(targets, predictions)\n",
    "\n",
    "print('RMSE (Train Dataset): ' + str(rmse_value_train))\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for row in test_matrix:\n",
    "    user, item, actual = row[0], row[1], row[2]\n",
    "    predictions.append(predict_naive(user, item))\n",
    "    targets.append(actual)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "targets = np.array(targets)\n",
    "rmse_value_test = rmse(targets, predictions)\n",
    "\n",
    "print('RMSE (Test Dataset): ' + str(rmse_value_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import *\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "data = pd.read_csv('test.csv')\n",
    "# data.rename(columns={'user_id':'book','book':'user_id', 'rating':'rating'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mimos\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# a) For a user x:\n",
    "# Find k similar users using cosine similarity and Pearson correlation. Use k = 5\n",
    "\n",
    "rows = data.user_id.unique()\n",
    "cols = data['book'].unique()\n",
    "data = data[['user_id', 'book', 'rating']]\n",
    "idict = dict(zip(cols, range(len(cols))))\n",
    "udict = dict(zip(rows, range(len(rows))))\n",
    "data.user_id = [ udict[i] for i in data.user_id ]\n",
    "data['book'] = [ idict[i] for i in data['book'] ]\n",
    "nmat = data.as_matrix()\n",
    "naive = np.zeros((len(rows),len(cols)))\n",
    "for row in nmat:\n",
    "    naive[row[0], row[1]] = row[2]\n",
    "amean = np.mean(naive[naive != 0])\n",
    "umean = sum(naive.T) / sum((naive != 0).T)\n",
    "imean = sum(naive) / sum((naive != 0))\n",
    "\n",
    "def cos(mat, a, b):\n",
    "    if a == b:\n",
    "        return 1\n",
    "    aval = mat.T[a].nonzero()\n",
    "    bval = mat.T[b].nonzero()\n",
    "    corated = np.intersect1d(aval, bval)\n",
    "    if len(corated) == 0:\n",
    "        return 0\n",
    "    avec = np.take(mat.T[a], corated)\n",
    "    bvec = np.take(mat.T[b], corated)\n",
    "    val = 1 - cosine(avec, bvec)\n",
    "    if np.isnan(val):\n",
    "        return 0\n",
    "    return val\n",
    "\n",
    "def pr(mat, a, b, imean):\n",
    "    if a == b:\n",
    "        return 1\n",
    "    aval = mat.T[a].nonzero()\n",
    "    bval = mat.T[b].nonzero()\n",
    "    corated = np.intersect1d(aval, bval)\n",
    "    if len(corated) < 2:\n",
    "        return 0\n",
    "    avec = np.take(mat.T[a], corated)\n",
    "    bvec = np.take(mat.T[b], corated)\n",
    "    avec1 = avec - imean[a]\n",
    "    bvec1 = bvec - imean[b]\n",
    "    val = 1 - cosine(avec1, bvec1)\n",
    "    if np.isnan(val):\n",
    "        return 0\n",
    "    return val\n",
    "\n",
    "def itemsimilar(mat, option=\"\"):\n",
    "    amean = np.mean(mat[mat!=0])\n",
    "    umean = sum(mat.T) / sum((mat!=0).T)\n",
    "    imean = sum(mat) / sum((mat!=0))\n",
    "    n = mat.shape[1]\n",
    "    sim_mat=np.zeros((n,n))\n",
    "    if option == 'pr':\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                sim_mat[i][j] = pr(mat, i, j, imean)\n",
    "        sim_mat = (sim_mat + 1)/2\n",
    "    elif option == 'cos':\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                sim_mat[i][j] = cos(mat, i, j)\n",
    "    else:\n",
    "        sim_mat = cosine_similarity(mat.T)\n",
    "    return sim_mat, amean, umean, imean\n",
    "\n",
    "sim_mat, amean, umean, imean = itemsimilar(naive,\"\")\n",
    "print('Similarity Matrix: ')\n",
    "print(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: 8.000000000000002\n",
      "Naive Matrix: \n",
      "[[7. 0. 0. ... 0. 0. 0.]\n",
      " [0. 8. 0. ... 0. 0. 0.]\n",
      " [0. 0. 8. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#b) Write a function to predict the rating for the user x and item j, based on K similar user to x.\n",
    "\n",
    "def predict(user, item, mat, item_similarity, amean, umean, imean, k=20):\n",
    "    nzero = mat[user].nonzero()[0]\n",
    "    if len(nzero) == 0:\n",
    "        return amean\n",
    "    baseline = imean + umean[user] - amean\n",
    "    choice = nzero[item_similarity[item, nzero].argsort()[::-1][:k]]\n",
    "    prediction = ((mat[user, choice] - baseline[choice]).dot(item_similarity[item, choice])/sum(item_similarity[item, choice]))+ baseline[item]\n",
    "    if np.isnan(prediction):\n",
    "        prediction = amean\n",
    "    if prediction > 10:\n",
    "        prediction = 10\n",
    "    if prediction < 1:\n",
    "        prediction = 1\n",
    "    return prediction\n",
    "\n",
    "prediction_value = predict(1, 1, naive, sim_mat, amean, umean, imean, k=5)\n",
    "print('Predicted Value: ' + str(prediction_value))\n",
    "print('Naive Matrix: ')\n",
    "print(naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mimos\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error\n",
      "RMSE : 0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mimos\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error\n",
      "RMSE : 0.5875\n"
     ]
    }
   ],
   "source": [
    "#c) Report the train and test RMSE errors for these predictions\n",
    "\n",
    "def get_results(train_data, test_data, option, k):\n",
    "    # train_data.rename(columns={'user_id':'book','book':'user_id', 'rating':'rating'}, inplace = True)\n",
    "    rows = train_data.user_id.unique()\n",
    "    cols = train_data['book'].unique()\n",
    "    train_data = train_data[['user_id', 'book', 'rating']]\n",
    "    idict = dict(zip(cols, range(len(cols))))\n",
    "    udict = dict(zip(rows, range(len(rows))))\n",
    "    train_data.user_id = [ udict[i] for i in train_data.user_id ]\n",
    "    train_data['book'] = [ idict[i] for i in train_data['book'] ]\n",
    "    mat = train_data.as_matrix()\n",
    "    full_mat = np.zeros((len(rows),len(cols)))\n",
    "    for row in mat:\n",
    "        full_mat[row[0], row[1]] = row[2]\n",
    "    item_similarity, amean, umean, imean = itemsimilar(full_mat, option)\n",
    "    preds=[]\n",
    "    real=[]\n",
    "    for i in mat:\n",
    "        preds.append(predict(i[0], i[1], full_mat, item_similarity, amean, umean, imean, k=5))\n",
    "        real.append(i[2])\n",
    "    err1 = mean_squared_error(preds,real)\n",
    "    print('Train Error')\n",
    "    print('RMSE : %.4f' % err1)\n",
    "\n",
    "    # test_data.rename(columns={'user_id':'book','book':'user_id', 'rating':'rating'}, inplace = True)\n",
    "    rows = test_data.user_id.unique()\n",
    "    cols = test_data['book'].unique()\n",
    "    test_data = test_data[['user_id', 'book', 'rating']]\n",
    "    idict = dict(zip(cols, range(len(cols))))\n",
    "    udict = dict(zip(rows, range(len(rows))))\n",
    "    test_data.user_id = [ udict[i] for i in test_data.user_id ]\n",
    "    test_data['book'] = [ idict[i] for i in test_data['book'] ]\n",
    "    mat = test_data.as_matrix()\n",
    "    full_mat = np.zeros((len(rows),len(cols)))\n",
    "    for row in mat:\n",
    "        full_mat[row[0], row[1]] = row[2]\n",
    "    item_similarity, amean, umean, imean = itemsimilar(full_mat, option)\n",
    "    preds=[]\n",
    "    real=[]\n",
    "    for i in mat:\n",
    "        preds.append(predict(i[0], i[1], full_mat, item_similarity, amean, umean, imean, k=5))\n",
    "        real.append(i[2])\n",
    "    err2 = mean_squared_error(preds,real)\n",
    "    print('Test Error')\n",
    "    print('RMSE : %.4f' % err2)\n",
    "    \n",
    "get_results(pd.read_csv(\"train.csv\"),pd.read_csv(\"test.csv\"), \"cos\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2d\n",
    "\n",
    "Firstly, the value of K is highly data dependent. In general, a larger k suppresses the effects of noise, but makes the classification boundaries less distinct.\n",
    "\n",
    "There are two approaches that are usually followed when it comes to choosing K values:\n",
    "1. Try many K values and use k-fold Cross-Validation to see which K value gives the best accuracy. The goal of cross-validation is to test the model’s ability to predict new data that was not used in estimating it, in order to flag problems like overfitting and to give an insight on how the model will generalize to an independent dataset. \n",
    "2. Alternatively, one can use the Elbow method. This method looks at the percentage of variance explained as a function of the number of clusters. One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
